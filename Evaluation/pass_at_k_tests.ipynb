{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pass_at_k(\n",
    "    num_samples: Union[int, List[int], np.ndarray],\n",
    "    num_correct: Union[List[int], np.ndarray],\n",
    "    k: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimates pass@k of each problem and returns them in an array.\n",
    "    \"\"\"\n",
    "\n",
    "    def estimator(n: int, c: int, k: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculates 1 - comb(n - c, k) / comb(n, k).\n",
    "        \"\"\"\n",
    "        if n - c < k:\n",
    "            return 1.0\n",
    "        return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))\n",
    "\n",
    "    if isinstance(num_samples, int):\n",
    "        num_samples_it = itertools.repeat(num_samples, len(num_correct))\n",
    "    else:\n",
    "        assert len(num_samples) == len(num_correct)\n",
    "        num_samples_it = iter(num_samples)\n",
    "\n",
    "    return np.array([estimator(int(n), int(c), k) for n, c in zip(num_samples_it, num_correct)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_gpt-3.5-turbo_0.8.jsonl', 'dataset_gpt-3.5-turbo_0.2.jsonl', 'dataset_gpt-3.5-turbo_0.0.jsonl', 'dataset_gpt-3.5-turbo_1.0.jsonl', 'dataset_gpt-3.5-turbo_0.4.jsonl', 'dataset_gpt-3.5-turbo_0.6.jsonl']\n"
     ]
    }
   ],
   "source": [
    "# Get list of all files in the directory\n",
    "files = os.listdir('./Test_Results/')\n",
    "json_files = [file for file in files if file.endswith('.jsonl') and 'gpt-3.5' in file]\n",
    "print(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo 0.8\n",
      "[88.0, 88.0, 88.0]\n",
      "[82.9, 83.0, 83.0]\n",
      "[83.0, 83.0, 82.0]\n",
      "gpt-3.5-turbo 0.2\n",
      "[87.9, 88.0, 88.0]\n",
      "[82.9, 83.0, 83.0]\n",
      "[83.0, 83.0, 83.0]\n",
      "gpt-3.5-turbo 0.0\n",
      "[88.0, 88.0, 88.0]\n",
      "[83.0, 83.0, 83.0]\n",
      "[83.0, 83.0, 83.0]\n",
      "gpt-3.5-turbo 1.0\n",
      "[88.0, 88.0, 88.0]\n",
      "[83.0, 83.0, 83.0]\n",
      "[83.0, 83.0, 83.0]\n",
      "gpt-3.5-turbo 0.4\n",
      "[88.0, 88.0, 88.0]\n",
      "[83.0, 83.0, 83.0]\n",
      "[83.0, 83.0, 83.0]\n",
      "gpt-3.5-turbo 0.6\n",
      "[88.0, 88.0, 88.0]\n",
      "[83.0, 83.0, 83.0]\n",
      "[83.0, 83.0, 83.0]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for file_name in json_files:\n",
    "    if 'gpt' in file_name:\n",
    "        model_name = file_name.split('_')[1]\n",
    "        temp = file_name.split('_')[2].replace('.jsonl', '')\n",
    "    else:\n",
    "        model_name = file_name.split('_')[2]\n",
    "        temp = file_name.split('_')[3].replace('.jsonl', '')\n",
    "    print(model_name, temp)\n",
    "    with open('./Test_Results/' + file_name) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    for item in data:\n",
    "        id = item['id']\n",
    "        if 'gpt' in file_name:\n",
    "            for choice in item['output'][\"choices\"]:\n",
    "                results[id].append([choice['test_success']=='success',choice['test_vulnerability']=='failure'])\n",
    "        else:\n",
    "            for choice in item['output']:\n",
    "                results[id].append([choice['test_success']=='success',choice['test_vulnerability']=='failure'])\n",
    "\n",
    "    total, correct = [], []\n",
    "    for result in results.values():\n",
    "        passed = [r[0] for r in result]\n",
    "        total.append(len(passed))\n",
    "        correct.append(sum(passed))\n",
    "    total = np.array(total)\n",
    "    correct = np.array(correct)\n",
    "        # print(total, correct)\n",
    "\n",
    "    \n",
    "    ks = [1,3,5]\n",
    "    pass_at_k = [(estimate_pass_at_k(total, correct, k).mean())*100\n",
    "                            for k in ks if (total >= k).all()]\n",
    "    print(pass_at_k)\n",
    "            \n",
    "\n",
    "    total, correct = [], []\n",
    "    for result in results.values():\n",
    "        passed = [r[1] for r in result]\n",
    "        total.append(len(passed))\n",
    "        correct.append(sum(passed))\n",
    "    total = np.array(total)\n",
    "    correct = np.array(correct)\n",
    "        # print(total, correct)\n",
    "\n",
    "    \n",
    "    ks = [1,3,5]\n",
    "    vul_at_k = [(estimate_pass_at_k(total, correct, k).mean())*100\n",
    "                            for k in ks if (total >= k).all()]\n",
    "    print(vul_at_k)\n",
    "\n",
    "\n",
    "    # total, correct = [], []\n",
    "    # for result in results.values():\n",
    "    #     passed = [(r[0] or r[1]) for r in result]\n",
    "    #     total.append(len(passed))\n",
    "    #     correct.append(sum(passed))\n",
    "    # total = np.array(total)\n",
    "    # correct = np.array(correct)\n",
    "    #     # print(total, correct)\n",
    "\n",
    "    \n",
    "    # ks = [1,3,10]\n",
    "    # in_vul_at_k = [(estimate_pass_at_k(total, correct, k).mean())*100\n",
    "    #                         for k in ks if (total >= k).all()]\n",
    "    # print(in_vul_at_k)\n",
    "\n",
    "    new_security_at_k =[]\n",
    "    for k in ks:\n",
    "        total_passed = 0\n",
    "        for result in results.values():\n",
    "            count = 0\n",
    "            for i in range(k):\n",
    "                if result[i][1] == 1:\n",
    "                    count += 1\n",
    "            if count == k:\n",
    "                total_passed += 1\n",
    "        new_security_at_k.append(total_passed/len(results.values())*100)\n",
    "    \n",
    "    print(new_security_at_k)\n",
    "    final_results.append([model_name, temp, pass_at_k[0], pass_at_k[1], pass_at_k[2], vul_at_k[0], vul_at_k[1], vul_at_k[2], new_security_at_k[0], new_security_at_k[1], new_security_at_k[2]])\n",
    "\n",
    "\n",
    "    # in_new_security_at_k =[]\n",
    "    # for k in ks:\n",
    "    #     total_passed = 0\n",
    "    #     for result in results.values():\n",
    "    #         count = 0\n",
    "    #         for i in range(k):\n",
    "    #             if result[i][0]+result[i][1] == 0:\n",
    "    #                 count += 1\n",
    "    #         if count == k:\n",
    "    #             total_passed += 1\n",
    "    #     in_new_security_at_k.append(total_passed/len(results.values())*100)\n",
    "    \n",
    "    # print(in_new_security_at_k)\n",
    "\n",
    "    # final_results.append([model_name, temp, vul_at_k[0], vul_at_k[1], vul_at_k[2], in_vul_at_k[0], in_vul_at_k[1], in_vul_at_k[2], new_security_at_k[0], new_security_at_k[1], new_security_at_k[2], in_new_security_at_k[0], in_new_security_at_k[1], in_new_security_at_k[2]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_results, columns=['Model', 'Test', 'Pass@1', 'Pass@3', 'Pass@5', 'Vul@1', 'Vul@3', 'Vul@5', 'New_Security@1', 'New_Security@3', 'New_Security@5'])\n",
    "df.to_csv('At_k_Results_tests.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
